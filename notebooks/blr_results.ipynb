{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression — German Credit\n",
    "\n",
    "Visualize and analyze ETD variants vs baselines on the Bayesian logistic regression posterior.\n",
    "\n",
    "**Prerequisites:**\n",
    "```bash\n",
    "# 1. Generate NUTS reference (one-time, ~5 min)\n",
    "python -m experiments.nuts --target blr --dataset german_credit --force\n",
    "\n",
    "# 2. Run the experiment\n",
    "python -m experiments.run configs/blr_german.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Project imports — chdir to project root so relative paths\n",
    "# (e.g., data/etd.duckdb, results/reference/) resolve correctly.\n",
    "ROOT = Path.cwd().parent\n",
    "os.chdir(ROOT)\n",
    "sys.path.insert(0, str(ROOT / \"src\"))\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from etd.targets.blr import BLRTarget\n",
    "from experiments.nuts import load_reference\n",
    "from figures.style import (\n",
    "    ALGO_COLORS, FULL_WIDTH, COL_WIDTH,\n",
    "    savefig_paper, setup_style,\n",
    ")\n",
    "\n",
    "setup_style()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# --- Find the latest results directory ---\n",
    "results_root = ROOT / \"results\" / \"blr-ionosphere\"\n",
    "assert results_root.exists(), (\n",
    "    f\"No results at {results_root}.\\n\"\n",
    "    \"Run: python -m experiments.run configs/blr/CONFIG_NAME.yaml\"\n",
    ")\n",
    "runs = sorted(results_root.iterdir())\n",
    "RESULTS_DIR = runs[-1]\n",
    "print(f\"Loading from: {RESULTS_DIR}\")\n",
    "\n",
    "# --- Load config ---\n",
    "with open(RESULTS_DIR / \"config.yaml\") as f:\n",
    "    exp_config = yaml.safe_load(f)\n",
    "\n",
    "exp = exp_config[\"experiment\"]\n",
    "target_cfg = exp[\"target\"]\n",
    "target_params = target_cfg.get(\"params\", {})\n",
    "\n",
    "# --- Load metrics ---\n",
    "with open(RESULTS_DIR / \"metrics.json\") as f:\n",
    "    raw_metrics = json.load(f)\n",
    "\n",
    "metrics = {}\n",
    "for seed, algo_dict in raw_metrics.items():\n",
    "    metrics[seed] = {}\n",
    "    for algo, ckpt_dict in algo_dict.items():\n",
    "        metrics[seed][algo] = {int(k): v for k, v in ckpt_dict.items()}\n",
    "\n",
    "# --- Load particles ---\n",
    "particles = dict(np.load(RESULTS_DIR / \"particles.npz\"))\n",
    "\n",
    "# --- Build target ---\n",
    "target = BLRTarget(**target_params)\n",
    "d = target.dim\n",
    "print(f\"Target: blr, dim={d}, prior_std={target.prior_std}\")\n",
    "print(f\"Data: X {target.X.shape}, y {target.y.shape} ({float(target.y.mean()):.1%} positive)\")\n",
    "\n",
    "# --- Constants ---\n",
    "SEEDS = [f\"seed{i}\" for i in exp[\"seeds\"]]\n",
    "CHECKPOINTS = sorted(set(\n",
    "    int(k) for algo_dict in metrics.values()\n",
    "    for ckpt_dict in algo_dict.values()\n",
    "    for k in ckpt_dict\n",
    "))\n",
    "ALL_ALGOS = sorted(set(\n",
    "    algo for algo_dict in metrics.values() for algo in algo_dict\n",
    "))\n",
    "ALGOS = ALL_ALGOS  # Keep all for BLR\n",
    "print(f\"Checkpoints: {CHECKPOINTS}\")\n",
    "print(f\"Algorithms: {ALGOS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load NUTS reference ---\n",
    "ref_samples = load_reference(\"blr\", target_params)\n",
    "if ref_samples is not None:\n",
    "    print(f\"NUTS reference: {ref_samples.shape}\")\n",
    "    ref_mean = ref_samples.mean(axis=0)\n",
    "    ref_std = ref_samples.std(axis=0)\n",
    "    HAS_REF = True\n",
    "else:\n",
    "    print(\"No NUTS reference found. Run:\")\n",
    "    print(\"  python -m experiments.nuts --target blr --dataset german_credit\")\n",
    "    HAS_REF = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convergence traces\n",
    "\n",
    "Each column is one algorithm; light gray traces show other algorithms for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_metric(metric_name, algos=None):\n",
    "    \"\"\"Return {algo: (median, q25, q75)} arrays over checkpoints.\"\"\"\n",
    "    if algos is None:\n",
    "        algos = ALGOS\n",
    "    out = {}\n",
    "    for algo in algos:\n",
    "        vals = []\n",
    "        for seed in SEEDS:\n",
    "            row = [\n",
    "                metrics.get(seed, {}).get(algo, {}).get(c, {}).get(metric_name, np.nan)\n",
    "                for c in CHECKPOINTS\n",
    "            ]\n",
    "            vals.append(row)\n",
    "        vals = np.array(vals)\n",
    "        out[algo] = (\n",
    "            np.nanmedian(vals, axis=0),\n",
    "            np.nanpercentile(vals, 25, axis=0),\n",
    "            np.nanpercentile(vals, 75, axis=0),\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "# Evenly-spaced x positions with checkpoint labels\n",
    "X_POS = np.arange(len(CHECKPOINTS))\n",
    "X_LABELS = [str(c) for c in CHECKPOINTS]\n",
    "\n",
    "# Sparse tick labels\n",
    "TICK_SHOW = set(range(0, len(CHECKPOINTS), max(1, len(CHECKPOINTS) // 5)))\n",
    "TICK_SHOW.add(len(CHECKPOINTS) - 1)  # always show last\n",
    "X_LABELS_SPARSE = [str(c) if i in TICK_SHOW else \"\" for i, c in enumerate(CHECKPOINTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_specs = [\n",
    "    (\"energy_distance\", \"Energy distance\"),\n",
    "    (\"sliced_wasserstein\", \"Sliced $W_2$\"),\n",
    "    (\"mean_rmse\", \"Mean RMSE\"),\n",
    "    (\"mean_error\", \"Mean error\"),\n",
    "]\n",
    "n_metrics = len(metric_specs)\n",
    "n_algos = len(ALGOS)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_metrics, n_algos,\n",
    "    figsize=(FULL_WIDTH, 1.5 * n_metrics),\n",
    "    sharex=True, sharey=\"row\",\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "for row, (metric_name, ylabel) in enumerate(metric_specs):\n",
    "    data = gather_metric(metric_name)\n",
    "    for col, algo in enumerate(ALGOS):\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Gray context: all OTHER algorithms\n",
    "        for other in ALGOS:\n",
    "            if other == algo:\n",
    "                continue\n",
    "            med_o, _, _ = data[other]\n",
    "            ax.plot(X_POS, med_o, color=\"#cccccc\", linewidth=0.6, zorder=1)\n",
    "\n",
    "        # Foreground: this algorithm's median + IQR\n",
    "        median, q25, q75 = data[algo]\n",
    "        color = ALGO_COLORS.get(algo, \"#333\")\n",
    "        ax.fill_between(X_POS, q25, q75, color=color, alpha=0.18, zorder=2)\n",
    "        ax.plot(X_POS, median, color=color, linewidth=1.5, zorder=3)\n",
    "\n",
    "        ax.set_xticks(X_POS)\n",
    "        ax.set_xticklabels(X_LABELS_SPARSE, fontsize=6)\n",
    "        ax.tick_params(direction=\"out\", length=3)\n",
    "\n",
    "        if row == 0:\n",
    "            ax.set_title(algo, fontsize=9)\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(ylabel, fontsize=8)\n",
    "        if row == n_metrics - 1:\n",
    "            ax.set_xlabel(\"Iteration\", fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convergence overlay\n",
    "\n",
    "All algorithms on the same axes for direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    1, len(metric_specs),\n",
    "    figsize=(FULL_WIDTH, 2.2),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "# Sparse tick indices — show ~5 labels to prevent overlap\n",
    "n_ckpts = len(CHECKPOINTS)\n",
    "step = max(1, n_ckpts // 4)\n",
    "tick_idx = sorted(set(list(range(0, n_ckpts, step)) + [n_ckpts - 1]))\n",
    "\n",
    "for ax, (metric_name, ylabel) in zip(axes, metric_specs):\n",
    "    data = gather_metric(metric_name)\n",
    "    for algo in ALGOS:\n",
    "        median, q25, q75 = data[algo]\n",
    "        color = ALGO_COLORS.get(algo, \"#333\")\n",
    "        is_baseline = algo in (\"SVGD\", \"ULA\", \"MPPI\", \"MALA\")\n",
    "        ls = \"--\" if is_baseline else \"-\"\n",
    "        ax.fill_between(X_POS, q25, q75, color=color, alpha=0.12)\n",
    "        ax.plot(X_POS, median, color=color, linewidth=1.5, linestyle=ls, label=algo)\n",
    "\n",
    "    ax.set_xticks([X_POS[i] for i in tick_idx])\n",
    "    ax.set_xticklabels([str(CHECKPOINTS[i]) for i in tick_idx], fontsize=7)\n",
    "    ax.set_ylabel(ylabel, fontsize=8)\n",
    "    ax.set_xlabel(\"Iteration\", fontsize=8)\n",
    "    ax.tick_params(direction=\"out\", length=3)\n",
    "\n",
    "axes[0].legend(fontsize=7, frameon=False, loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Marginal posterior ridgeline\n",
    "\n",
    "Ridgeline plot for the most informative dimensions (largest NUTS posterior std).\n",
    "Each row shows one dimension: NUTS reference as a gray fill, algorithm KDEs overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dimensions to display: top-6 by NUTS posterior std (or particle spread)\n",
    "final_ckpt = max(CHECKPOINTS)\n",
    "N_DIMS_SHOW = 6\n",
    "\n",
    "if HAS_REF:\n",
    "    dim_order = np.argsort(-ref_std)  # largest std first\n",
    "else:\n",
    "    # Fallback: use particle spread from first algorithm\n",
    "    key0 = f\"seed0__{ALGOS[0]}__iter{final_ckpt}\"\n",
    "    fallback_std = particles[key0].std(axis=0)\n",
    "    dim_order = np.argsort(-fallback_std)\n",
    "\n",
    "DIMS_SHOW = dim_order[:N_DIMS_SHOW]\n",
    "print(f\"Displaying dimensions: {DIMS_SHOW}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    N_DIMS_SHOW, 1,\n",
    "    figsize=(COL_WIDTH * 1.8, 1.0 * N_DIMS_SHOW),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "for row, dim_idx in enumerate(DIMS_SHOW):\n",
    "    ax = axes[row]\n",
    "\n",
    "    # --- Determine x range ---\n",
    "    if HAS_REF:\n",
    "        ref_dim = ref_samples[:, dim_idx]\n",
    "        pad = 0.3 * ref_dim.std()\n",
    "        x_lo, x_hi = ref_dim.min() - pad, ref_dim.max() + pad\n",
    "    else:\n",
    "        all_vals = []\n",
    "        for algo in ALGOS:\n",
    "            key = f\"seed0__{algo}__iter{final_ckpt}\"\n",
    "            if key in particles:\n",
    "                all_vals.append(particles[key][:, dim_idx])\n",
    "        combined = np.concatenate(all_vals)\n",
    "        pad = 0.3 * combined.std()\n",
    "        x_lo, x_hi = combined.min() - pad, combined.max() + pad\n",
    "\n",
    "    x_grid = np.linspace(x_lo, x_hi, 300)\n",
    "\n",
    "    # --- NUTS reference ---\n",
    "    if HAS_REF:\n",
    "        kde_ref = stats.gaussian_kde(ref_dim)\n",
    "        ref_density = kde_ref(x_grid)\n",
    "        ax.fill_between(\n",
    "            x_grid, ref_density, color=\"#DDDDDD\", alpha=0.6,\n",
    "            label=\"NUTS\" if row == 0 else None,\n",
    "        )\n",
    "        ax.plot(x_grid, ref_density, color=\"#AAAAAA\", linewidth=0.7)\n",
    "\n",
    "    # --- Algorithm KDEs (seed 0) ---\n",
    "    for algo in ALGOS:\n",
    "        key = f\"seed0__{algo}__iter{final_ckpt}\"\n",
    "        if key not in particles:\n",
    "            continue\n",
    "        vals = particles[key][:, dim_idx]\n",
    "        kde = stats.gaussian_kde(vals)\n",
    "        color = ALGO_COLORS.get(algo, \"#333\")\n",
    "        is_baseline = algo in (\"SVGD\", \"ULA\", \"MPPI\", \"MALA\")\n",
    "        ax.plot(\n",
    "            x_grid, kde(x_grid),\n",
    "            color=color, linewidth=1.2,\n",
    "            linestyle=\"--\" if is_baseline else \"-\",\n",
    "            label=algo if row == 0 else None,\n",
    "        )\n",
    "\n",
    "    # --- Styling ---\n",
    "    ax.set_ylabel(f\"dim {dim_idx}\", fontsize=8, rotation=0, labelpad=28, va=\"center\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(x_lo, x_hi)\n",
    "\n",
    "    # Spine cleanup: only keep bottom spine on the last row\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(row == N_DIMS_SHOW - 1)\n",
    "    if row < N_DIMS_SHOW - 1:\n",
    "        ax.tick_params(bottom=False, labelbottom=False)\n",
    "    else:\n",
    "        ax.tick_params(direction=\"out\", length=3)\n",
    "        ax.set_xlabel(r\"$\\theta$\", fontsize=9)\n",
    "\n",
    "axes[0].legend(fontsize=7, frameon=False, ncol=min(len(ALGOS), 4), loc=\"upper right\")\n",
    "fig.suptitle(\"Marginal posteriors (seed 0, final iteration)\", fontsize=10, y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Coefficient error heatmap\n\nSigned error of each algorithm's posterior mean relative to the NUTS reference,\nper dimension. Blue = algorithm underestimates, red = overestimates, white = agreement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if HAS_REF:\n    # --- Compute per-algorithm signed mean error vs NUTS ---\n    mean_errors = np.full((d, len(ALGOS)), np.nan)\n\n    for j, algo in enumerate(ALGOS):\n        means_per_seed = []\n        for seed_idx in range(len(SEEDS)):\n            key = f\"seed{seed_idx}__{algo}__iter{final_ckpt}\"\n            if key not in particles:\n                continue\n            means_per_seed.append(particles[key].mean(axis=0))\n\n        if means_per_seed:\n            algo_mean = np.median(means_per_seed, axis=0)\n            mean_errors[:, j] = algo_mean - ref_mean\n\n    # Transpose: (d, n_algos) -> (n_algos, d) — wide layout\n    mean_errors_T = mean_errors.T\n\n    # Signed square-root transform to compress outliers\n    err_transformed = np.sign(mean_errors_T) * np.sqrt(np.abs(mean_errors_T))\n    vmax_err = max(np.nanmax(np.abs(err_transformed)), 0.1)\n\n    # Sparse feature-index labels\n    feat_labels_sparse = [\n        str(i) if (i % max(1, d // 16) == 0 or i == d - 1) else \"\"\n        for i in range(d)\n    ]\n\n    n_algos = len(ALGOS)\n    fig, ax = plt.subplots(\n        figsize=(FULL_WIDTH, n_algos * 0.4 + 0.8),\n        constrained_layout=True,\n    )\n\n    im = ax.imshow(\n        err_transformed,\n        aspect=\"auto\",\n        cmap=\"RdBu_r\",\n        vmin=-vmax_err,\n        vmax=vmax_err,\n        interpolation=\"nearest\",\n    )\n    ax.set_yticks(np.arange(n_algos))\n    ax.set_yticklabels(ALGOS, fontsize=7)\n    ax.set_xticks(np.arange(d))\n    ax.set_xticklabels(feat_labels_sparse, fontsize=6)\n    ax.set_xlabel(\"Feature index\", fontsize=8)\n    ax.set_title(\"Posterior mean error vs NUTS\", fontsize=9, loc=\"left\")\n\n    cb = fig.colorbar(im, ax=ax, shrink=0.7, pad=0.02, aspect=15)\n    cb.ax.tick_params(labelsize=7)\n    cb_ticks = cb.get_ticks()\n    cb.set_ticklabels([f\"{np.sign(t)*t**2:.2f}\" for t in cb_ticks])\n    cb.set_label(\"Signed error\", fontsize=8)\n\n    plt.show()\nelse:\n    print(\"Skipping coefficient heatmap — no NUTS reference.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-dimension variance ratio\n",
    "\n",
    "Ratio of particle variance to NUTS reference variance per dimension.\n",
    "Values < 1 indicate under-dispersion (common failure mode of VI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_REF:\n",
    "    fig, axes = plt.subplots(\n",
    "        1, len(ALGOS),\n",
    "        figsize=(FULL_WIDTH, 2.0),\n",
    "        sharey=True,\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "\n",
    "    ref_var = ref_samples.var(axis=0)  # (d,)\n",
    "\n",
    "    for ax, algo in zip(axes, ALGOS):\n",
    "        ratios_per_seed = []\n",
    "        for seed_idx in range(len(SEEDS)):\n",
    "            key = f\"seed{seed_idx}__{algo}__iter{final_ckpt}\"\n",
    "            if key not in particles:\n",
    "                continue\n",
    "            pts_var = particles[key].var(axis=0)\n",
    "            ratios_per_seed.append(pts_var / np.maximum(ref_var, 1e-10))\n",
    "\n",
    "        if not ratios_per_seed:\n",
    "            continue\n",
    "\n",
    "        median_ratio = np.median(ratios_per_seed, axis=0)\n",
    "        color = ALGO_COLORS.get(algo, \"#333\")\n",
    "\n",
    "        ax.bar(\n",
    "            np.arange(d), median_ratio,\n",
    "            color=color, alpha=0.7, width=0.7,\n",
    "        )\n",
    "        ax.axhline(1.0, color=\"#999\", linewidth=0.8, linestyle=\"--\")\n",
    "        ax.set_title(algo, fontsize=9)\n",
    "        ax.set_xlabel(\"Dimension\", fontsize=8)\n",
    "        ax.tick_params(direction=\"out\", length=3)\n",
    "        ax.set_xticks(np.arange(0, d, max(1, d // 5)))\n",
    "\n",
    "    axes[0].set_ylabel(\"Var ratio (particle / NUTS)\", fontsize=8)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping variance ratio plot — no NUTS reference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pairwise posterior structure\n",
    "\n",
    "2D scatter plots for selected pairs of dimensions to see how well\n",
    "algorithms capture posterior correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 3 pairs: the most correlated dimensions (from NUTS or particles)\n",
    "N_PAIRS = 3\n",
    "\n",
    "if HAS_REF:\n",
    "    corr_mat = np.corrcoef(ref_samples.T)  # (d, d)\n",
    "else:\n",
    "    key0 = f\"seed0__{ALGOS[0]}__iter{final_ckpt}\"\n",
    "    corr_mat = np.corrcoef(particles[key0].T)\n",
    "\n",
    "# Zero out diagonal and take absolute value\n",
    "np.fill_diagonal(corr_mat, 0)\n",
    "abs_corr = np.abs(corr_mat)\n",
    "\n",
    "# Find top pairs\n",
    "pairs = []\n",
    "used = set()\n",
    "flat_idx = np.argsort(abs_corr.ravel())[::-1]\n",
    "for idx in flat_idx:\n",
    "    i, j = divmod(idx, d)\n",
    "    if i >= j:  # upper triangle only\n",
    "        continue\n",
    "    if i in used and j in used:\n",
    "        continue\n",
    "    pairs.append((i, j))\n",
    "    used.update([i, j])\n",
    "    if len(pairs) == N_PAIRS:\n",
    "        break\n",
    "\n",
    "print(f\"Most correlated pairs: {pairs}\")\n",
    "for i, j in pairs:\n",
    "    print(f\"  dims ({i}, {j}): corr = {corr_mat[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    len(ALGOS), N_PAIRS,\n",
    "    figsize=(COL_WIDTH * 1.8, 1.4 * len(ALGOS)),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "if len(ALGOS) == 1:\n",
    "    axes = axes[None, :]  # ensure 2D\n",
    "\n",
    "for row, algo in enumerate(ALGOS):\n",
    "    for col, (di, dj) in enumerate(pairs):\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # NUTS reference (small gray dots)\n",
    "        if HAS_REF:\n",
    "            subsample = ref_samples[::4]  # thin for speed\n",
    "            ax.scatter(\n",
    "                subsample[:, di], subsample[:, dj],\n",
    "                s=4, color=\"#CCCCCC\", alpha=0.3, zorder=1, rasterized=True,\n",
    "            )\n",
    "\n",
    "        # Algorithm particles (seed 0)\n",
    "        key = f\"seed0__{algo}__iter{final_ckpt}\"\n",
    "        if key in particles:\n",
    "            pts = particles[key]\n",
    "            color = ALGO_COLORS.get(algo, \"#333\")\n",
    "            ax.scatter(\n",
    "                pts[:, di], pts[:, dj],\n",
    "                s=12, color=color, edgecolors=\"white\", linewidths=0.4,\n",
    "                zorder=5, alpha=0.8,\n",
    "            )\n",
    "\n",
    "        if row == 0:\n",
    "            ax.set_title(f\"dims ({di}, {dj})\", fontsize=8)\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(algo, fontsize=8)\n",
    "        ax.tick_params(direction=\"out\", length=2, labelsize=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predictive performance\n",
    "\n",
    "Posterior predictive accuracy and calibration on the training data.\n",
    "For each algorithm, compute $p(y=1 \\mid x) = \\frac{1}{N}\\sum_i \\sigma(x \\cdot \\theta_i)$\n",
    "using the final particles as an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "X_np = np.array(target.X)\n",
    "y_np = np.array(target.y)\n",
    "\n",
    "\n",
    "def posterior_predictive(pts, X):\n",
    "    \"\"\"Compute ensemble predictive probabilities.\n",
    "\n",
    "    Args:\n",
    "        pts: Particles, shape (N, d).\n",
    "        X: Design matrix, shape (n_data, d).\n",
    "\n",
    "    Returns:\n",
    "        Predicted probabilities, shape (n_data,).\n",
    "    \"\"\"\n",
    "    logits = X @ pts.T  # (n_data, N)\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))  # sigmoid\n",
    "    return probs.mean(axis=1)  # (n_data,)\n",
    "\n",
    "\n",
    "print(f\"{'Algorithm':<15} {'Accuracy':>10} {'Brier':>10}\")\n",
    "print(\"-\" * 37)\n",
    "\n",
    "pred_results = {}\n",
    "for algo in ALGOS:\n",
    "    accs, briers = [], []\n",
    "    for seed_idx in range(len(SEEDS)):\n",
    "        key = f\"seed{seed_idx}__{algo}__iter{final_ckpt}\"\n",
    "        if key not in particles:\n",
    "            continue\n",
    "        p_pred = posterior_predictive(particles[key], X_np)\n",
    "        acc = float(np.mean((p_pred >= 0.5) == y_np))\n",
    "        brier = float(brier_score_loss(y_np, p_pred))\n",
    "        accs.append(acc)\n",
    "        briers.append(brier)\n",
    "\n",
    "    pred_results[algo] = {\"accuracy\": accs, \"brier\": briers}\n",
    "    print(\n",
    "        f\"{algo:<15} {np.median(accs):>9.3f}  {np.median(briers):>9.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration plot: predicted probability vs observed frequency\n",
    "N_BINS = 10\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(ALGOS),\n",
    "    figsize=(FULL_WIDTH, 2.0),\n",
    "    sharex=True, sharey=True,\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "for ax, algo in zip(axes, ALGOS):\n",
    "    # Use seed 0\n",
    "    key = f\"seed0__{algo}__iter{final_ckpt}\"\n",
    "    if key not in particles:\n",
    "        continue\n",
    "    p_pred = posterior_predictive(particles[key], X_np)\n",
    "\n",
    "    # Bin predictions\n",
    "    bin_edges = np.linspace(0, 1, N_BINS + 1)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    observed_freq = np.zeros(N_BINS)\n",
    "    bin_counts = np.zeros(N_BINS)\n",
    "\n",
    "    for b in range(N_BINS):\n",
    "        mask = (p_pred >= bin_edges[b]) & (p_pred < bin_edges[b + 1])\n",
    "        if b == N_BINS - 1:\n",
    "            mask |= (p_pred == bin_edges[b + 1])  # include right edge\n",
    "        if mask.sum() > 0:\n",
    "            observed_freq[b] = y_np[mask].mean()\n",
    "            bin_counts[b] = mask.sum()\n",
    "\n",
    "    color = ALGO_COLORS.get(algo, \"#333\")\n",
    "    valid = bin_counts > 0\n",
    "    ax.bar(\n",
    "        bin_centers[valid], observed_freq[valid],\n",
    "        width=0.08, color=color, alpha=0.7,\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"#999\", linewidth=0.8)\n",
    "    ax.set_title(algo, fontsize=9)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.tick_params(direction=\"out\", length=3)\n",
    "\n",
    "axes[0].set_ylabel(\"Observed frequency\", fontsize=8)\n",
    "axes[len(ALGOS) // 2].set_xlabel(\"Predicted probability\", fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Variance ratio over iterations\n",
    "\n",
    "Track the median variance ratio across dimensions over the course of optimization.\n",
    "This reveals whether algorithms converge to the correct posterior spread or collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"variance_ratio_ref\" in exp.get(\"metrics\", []):\n",
    "    vr_data = gather_metric(\"variance_ratio_ref\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(COL_WIDTH * 1.5, 2.0))\n",
    "\n",
    "    for algo in ALGOS:\n",
    "        median, q25, q75 = vr_data[algo]\n",
    "        color = ALGO_COLORS.get(algo, \"#333\")\n",
    "        is_baseline = algo in (\"SVGD\", \"ULA\", \"MPPI\")\n",
    "        ls = \"--\" if is_baseline else \"-\"\n",
    "        ax.fill_between(X_POS, q25, q75, color=color, alpha=0.10)\n",
    "        ax.plot(X_POS, median, color=color, linewidth=1.5, linestyle=ls, label=algo)\n",
    "\n",
    "    ax.axhline(1.0, color=\"#999\", linewidth=0.8, linestyle=\"--\", label=\"Ideal\")\n",
    "    ax.set_xticks(X_POS)\n",
    "    ax.set_xticklabels(X_LABELS_SPARSE, fontsize=7)\n",
    "    ax.set_ylabel(\"Median variance ratio\", fontsize=8)\n",
    "    ax.set_xlabel(\"Iteration\", fontsize=8)\n",
    "    ax.legend(fontsize=7, frameon=False)\n",
    "    ax.tick_params(direction=\"out\", length=3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"variance_ratio_ref not in metrics config — skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final metrics summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = exp.get(\"metrics\", [\"energy_distance\", \"mean_error\"])\n",
    "\n",
    "print(f\"{'Algorithm':<15}\", end=\"\")\n",
    "for m in metric_names:\n",
    "    print(f\"  {m:<28}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (15 + 30 * len(metric_names)))\n",
    "\n",
    "for algo in ALGOS:\n",
    "    print(f\"{algo:<15}\", end=\"\")\n",
    "    for m in metric_names:\n",
    "        vals = [\n",
    "            metrics[s].get(algo, {}).get(final_ckpt, {}).get(m, np.nan)\n",
    "            for s in SEEDS\n",
    "        ]\n",
    "        valid = [v for v in vals if not np.isnan(v)]\n",
    "        if valid:\n",
    "            med = np.median(valid)\n",
    "            iqr_lo = np.percentile(valid, 25)\n",
    "            iqr_hi = np.percentile(valid, 75)\n",
    "            print(f\"  {med:>8.4f}  ({iqr_lo:.4f}\\u2013{iqr_hi:.4f})  \", end=\"\")\n",
    "        else:\n",
    "            print(f\"  {'N/A':>28}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Log-posterior evaluation\n",
    "\n",
    "Mean log-posterior of particles over iterations — a quick sanity check\n",
    "that particles are moving toward high-probability regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(COL_WIDTH * 1.5, 2.0))\n",
    "\n",
    "for algo in ALGOS:\n",
    "    lp_per_seed = []\n",
    "    for seed_idx in range(len(SEEDS)):\n",
    "        lp_ckpts = []\n",
    "        for ckpt in CHECKPOINTS:\n",
    "            key = f\"seed{seed_idx}__{algo}__iter{ckpt}\"\n",
    "            if key in particles:\n",
    "                pts = jnp.array(particles[key])\n",
    "                lp = float(target.log_prob(pts).mean())\n",
    "            else:\n",
    "                lp = np.nan\n",
    "            lp_ckpts.append(lp)\n",
    "        lp_per_seed.append(lp_ckpts)\n",
    "\n",
    "    lp_arr = np.array(lp_per_seed)\n",
    "    median = np.nanmedian(lp_arr, axis=0)\n",
    "    q25 = np.nanpercentile(lp_arr, 25, axis=0)\n",
    "    q75 = np.nanpercentile(lp_arr, 75, axis=0)\n",
    "\n",
    "    color = ALGO_COLORS.get(algo, \"#333\")\n",
    "    is_baseline = algo in (\"SVGD\", \"ULA\", \"MPPI\")\n",
    "    ls = \"--\" if is_baseline else \"-\"\n",
    "    ax.fill_between(X_POS, q25, q75, color=color, alpha=0.10)\n",
    "    ax.plot(X_POS, median, color=color, linewidth=1.5, linestyle=ls, label=algo)\n",
    "\n",
    "# NUTS reference log-posterior\n",
    "if HAS_REF:\n",
    "    ref_lp = float(target.log_prob(jnp.array(ref_samples)).mean())\n",
    "    ax.axhline(ref_lp, color=\"#999\", linewidth=0.8, linestyle=\":\", label=\"NUTS mean\")\n",
    "\n",
    "ax.set_xticks(X_POS)\n",
    "ax.set_xticklabels(X_LABELS_SPARSE, fontsize=7)\n",
    "ax.set_ylabel(\"Mean log-posterior\", fontsize=8)\n",
    "ax.set_xlabel(\"Iteration\", fontsize=8)\n",
    "ax.legend(fontsize=7, frameon=False)\n",
    "ax.tick_params(direction=\"out\", length=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}